## 多层感知机
_定义_：多层感知机是采用多个神经网络层，并在其中添加了非线性单元，从而使得网络可以得到从输入到输出的非线性映射，从而将线性模型转化为非线性模型，在此基础上，增加网络的深度可以进一步增加网络的复杂度。

### 激活函数：
_定义_：拥有非线性部分的函数，即函数的导数并非处处一致，常见的激活函数有Relu，Leaky-relu，tanh，sigmoid等

实例：


<div align=center>  

![alt text](https://github.com/shiyutang/Hands-on-deep-learning/blob/master/%E5%9B%BE%E7%89%87/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png "softmax 常数不变性")   

</div>


### 重要的问题：
#### 1. 不同的激活函数应当如何选择？

* Relu：由于其计算效率和不容易梯度消失，从而大多数情况下使用；但是，由于其输出范围不确定，因此只能在隐藏层中使用；同时Relu函数因为在小于0的部分为0，因此容易在学习率大时使神经元失活

* Sigmoid函数：用于分类器时，通常效果更好，但是存在由于梯度消失。

* Tanh 函数: 和sigmoid 相似，但是其导数输出在（0，1），相比sigmoid更容易激活神经元；也存在梯度消失的问题

